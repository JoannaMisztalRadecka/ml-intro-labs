{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5-linear-classifier-and-neural-network.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UGMSGcyrWdv8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyDSx1Mx08y"
      },
      "source": [
        "# Linear models and Neural Networks\n",
        "In this notebook, we will learn what is the difference between a linear model and a neural network. We will use both models for predictions on an articial dataset and for hand written digits classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMSGcyrWdv8"
      },
      "source": [
        "#### Imports and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-gYwDWn7eSh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import math \n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_blobs, make_regression\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6IqMQ8RMd2a"
      },
      "source": [
        "def display_confusion_matrix(y_test, y_pred):\n",
        "  confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
        "  confusion_matrix.index.name = 'Actual'\n",
        "  confusion_matrix.columns.name = 'Predicted'\n",
        "  sns.heatmap(confusion_matrix, annot=True)\n",
        "\n",
        "def plot_decision_boundary(X, y, clf: BaseEstimator):\n",
        "  xx, yy = np.mgrid[0:1:.01, 0:1:.001]\n",
        "  grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "  f, ax = plt.subplots(figsize=(8, 6))\n",
        "  probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "  contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
        "                        vmin=0, vmax=1)\n",
        "  ax_c = f.colorbar(contour)\n",
        "  ax_c.set_label(\"$P(y = 1)$\")\n",
        "  ax_c.set_ticks([0, .25, .5, .75, 1])\n",
        "\n",
        "  ax.scatter(X[100:,0], X[100:, 1], c=y[100:], s=50,\n",
        "            cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "            edgecolor=\"white\", linewidth=1)\n",
        "\n",
        "  ax.set(aspect=\"equal\",\n",
        "        xlim=(0, 1), ylim=(0, 1),\n",
        "        xlabel=\"$X_1$\", ylabel=\"$X_2$\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCG9yV8c1Kx"
      },
      "source": [
        "## Regression\n",
        "\n",
        "First, we will learn how a linear regression model works. To understand this, let us first generate some articial data of two dimensions - `x` is the feature and `y` is the predicted value. Our goal will be to build a model which correctly predicts the value of `y` given `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnY2uHj2XURb"
      },
      "source": [
        "x, y = make_regression(n_samples=1000, n_features=1, noise=30, bias=2, random_state=0)\n",
        "x = x[:, 0]\n",
        "sns.scatterplot(x=x, y=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdhWV55ydU75"
      },
      "source": [
        "### Manual selection of linear coefficients\n",
        "\n",
        "Now, let us try to manually find the best line which fits this dataset.\n",
        "To do this, we will try to \"guess\" the coefficients of a linear function which defines the predictions.\n",
        "\n",
        "In our case, we have only 1 feature `x`, so we will try to find coefficients $a$ and $b$ for equation $y=ax+b$.\n",
        "\n",
        "When selecting the coefficients, we will try to minimize the mean squarred prediction error.\n",
        "\n",
        "Try to find the best coefficients by starting with $a=0, b=0$. Is it a good fit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP25J8PpaDtN"
      },
      "source": [
        "a = ??\n",
        "b = ??\n",
        "y_pred = x * a + b\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "sns.scatterplot(x=x, y=y)\n",
        "sns.lineplot(x=x, y=y_pred, color='r').set_title(f\"$y={a}x+{b}$, MSE: {mse:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbazYHMJeVTf"
      },
      "source": [
        "Now, try to gradually modify the values of `a` and `b`. Can you find the best fit which minimizes the error?\n",
        "\n",
        "We can plot the error for different values of parameter $a$. The optimal values of parameters could be found by finding the coefficients that minimize this error (loss) function. However, when there are many variables, such a direct optimization would be inefficient and approximate gradient optimization methods are applied instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd-wToPgN6pU"
      },
      "source": [
        "b = 0\n",
        "a_grid = np.linspace(-100, 200)\n",
        "mse_for_a_grid = []\n",
        "\n",
        "for a in a_grid:\n",
        "  y_pred = x * a + b\n",
        "  mse = mean_squared_error(y, y_pred)\n",
        "  mse_for_a_grid.append(mse)\n",
        "sns.lineplot(x=a_grid, y=mse_for_a_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgUU4e8POH8g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAunXhA0iJ1O"
      },
      "source": [
        "### Linear regression model\n",
        "\n",
        "When choosing the best parameters, you started with some random guess, next, you looked at the results and you updated the parameters a little bit until the classification error decreased. \n",
        "\n",
        "A simple linear model which is often used for regression tasks is the *linear regression* model. The learning works in a similar way - it tries to choose the parameters of a linear function: $$\\hat{y}=a_1x_1 + a_2x_2 \\ldots +a_nx_n +b$$\n",
        "by minimizing the squarred error between the true and predicted value of $y$. After each iteration, it calculates the errors and updated weights according to the gradients of the objective function. This algorithm is called *gradient descent*.\n",
        "\n",
        "Let us check how the model fits to the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKpAepx_76i"
      },
      "source": [
        "x_reshaped = x.reshape(-1, 1)\n",
        "lr = LinearRegression(fit_intercept=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpJFT1eI_-ME"
      },
      "source": [
        "Fit the model and calculate predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkZ7XR7jABbV"
      },
      "source": [
        "lr.fit(x_reshaped, y)\n",
        "y_pred = lr.predict(x_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgqHV7TPf6kP"
      },
      "source": [
        "mse = mean_squared_error(y, y_pred)\n",
        "sns.scatterplot(x=x, y=y)\n",
        "sns.lineplot(x=x, y=y_pred, color='r').set_title(\n",
        "    f\"$y={lr.coef_[0]:.2f}x+{lr.intercept_:.2f}$, MSE: {mse:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHCwC43Xi4gL"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Next, we will compare the performance of linear and non-linear models on the classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmB7mAYPgZj"
      },
      "source": [
        "### What is a linearly separable problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9dkbHbfqo4z"
      },
      "source": [
        "To understand the difference between linear and non-linear models, let us generate some artificial data points for a binary classification problem.\n",
        "\n",
        "Our goal will be to separate the two classes (0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XanjmLAZpha8"
      },
      "source": [
        "X, y = make_blobs(n_samples=1000, n_features=2, centers=2, cluster_std=3,\n",
        "                  random_state=1)\n",
        "X = minmax_scale(X)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lDPwmepQd0N"
      },
      "source": [
        "### Linear classification with logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NybZtJXpOFvY"
      },
      "source": [
        "A linear model which is often used for classification is the *logistic regression* model. It applies a logistic (sigmoid) function to the output of linear function to calculate the probablity of each class for a given point:\n",
        "\n",
        "$$ \\sigma(z)= \\frac{1}{1-e^{-z}}$$\n",
        "\n",
        "Next, the predictions are classified as positive if this score is above a theshold (typically 0.5)\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxma8QahRod-"
      },
      "source": [
        "x = np.linspace(-10, 10, 100) \n",
        "z = 1/(1 + np.exp(-x)) \n",
        "  \n",
        "sns.lineplot(x, z) \n",
        "plt.xlabel(\"z\") \n",
        "plt.ylabel(\"$\\sigma(z)$\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT4FBqZGR-H6"
      },
      "source": [
        "Let us apply the logistic classifier to this binary classification problem.\n",
        "\n",
        "Split the dataset into train and test sets and calculate the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn0zAHwQs2qz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
        "lr = LogisticRegression()\n",
        "lr.fit(??)\n",
        "y_pred = lr.predict(??)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1YA9NANAove"
      },
      "source": [
        "Next, we will plot the decision boundary of the model (its prediction score for every point on the grid)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAeMXxmSRNJV"
      },
      "source": [
        "plot_decision_boundary(X, y, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMAYySrBR1AY"
      },
      "source": [
        "display_confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqCRJHItEUY"
      },
      "source": [
        "Since the model fits linear coefficients, it is possible to display the resulting weights to verify which ones have the highest impact on prediction (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GclOXLrVuyna"
      },
      "source": [
        "pd.Series(lr.coef_[0], index=['x1', 'x2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-O_-dVjTWRJ"
      },
      "source": [
        "### Non-linearly separable problem\n",
        "\n",
        "In the previous example, we could find the line which separates the examples. Now, let's look at a different dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZN6G1k7UmtV"
      },
      "source": [
        "X, y = make_moons(n_samples=5000, noise=0.3, random_state=0)\n",
        "X = minmax_scale(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4yJhcmLTyVH"
      },
      "source": [
        "Could you plot a line which separates these two classes? \n",
        "\n",
        "Now, let's fit a logistic regression model on the train set and calculate the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKn5ed9hT8Kk"
      },
      "source": [
        "lr = ??\n",
        "??\n",
        "y_pred = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxS0AO3EUUN_"
      },
      "source": [
        "Plot the decision boundary for this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsth0AwcTsL9"
      },
      "source": [
        "plot_decision_boundary(X_test, y_test, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT_1PHPeSQoC"
      },
      "source": [
        "display_confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls1JIWyEUZr9"
      },
      "source": [
        "We can see that in this case, the accuracy is much lower and the line does not separate all the points. \n",
        "\n",
        "This is because this dataset is not **linearly separable**. To find a proper decision boundary, we will need a non-linear model which can learn more complex features, such as a neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpEHqc_o1j8"
      },
      "source": [
        "## Neural network classification\n",
        "\n",
        "To see the difference between a linear classifier and a neural network, let us take a look at the following examples of model training for different architectures and datasets. \n",
        "\n",
        "In each example, look at:\n",
        "- Look at the train and test loss after (approx.) 10, 100 and 1000 epochs.\n",
        "- Modify the learning rate parameter (check the values of 0.01, 0.1 and 1). How did the loss change after these epochs?\n",
        "\n",
        "\n",
        "\n",
        "[Example 1](https://playground.tensorflow.org/#activation=sigmoid&regularization=L2&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.001&noise=15&networkShape=&seed=0.32144&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false): linearly separable problem, linear model and features\n",
        "\n",
        "[Example 2](https://playground.tensorflow.org/#activation=sigmoid&regularization=L2&batchSize=16&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.001&noise=15&networkShape=&seed=0.98211&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false): non-linearly separable problem, linear model and features\n",
        "\n",
        "[Example 3](https://playground.tensorflow.org/#activation=sigmoid&regularization=L2&batchSize=16&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.001&noise=15&networkShape=&seed=0.98211&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false): non-linearly separable problem, linear model, non-linear features\n",
        "\n",
        "[Example 4](https://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=16&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.001&noise=10&networkShape=4,3,1&seed=0.87204&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false): non-linearly separable problem, non-linear model, linear features.\n",
        "\n",
        "Analyze what features were learnt by each neuron of the multi-layer model. What can you observe about the features in each layer?\n",
        "\n",
        "Try to experiment with other datasets and input features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWQFCRNLP7m1"
      },
      "source": [
        "### Neural network implementation\n",
        "\n",
        "Next, we will implement the neural network architecture which learns the non-linear features for the previous dataset. We will use `MLPClassifier` class from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw34UUwjPdyg"
      },
      "source": [
        "nn =  MLPClassifier(solver='lbfgs', alpha=1, random_state=1, max_iter=5000,\n",
        "            hidden_layer_sizes=[20, 10],  learning_rate_init=0.01, \n",
        "            early_stopping=True)\n",
        "?? # fit the model on training set\n",
        "?? # print the accuracy on the test set\n",
        "?? # plot the decision boundary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkBmzNqUQaSC"
      },
      "source": [
        "Try to modify the network architecture by changing the number of neurons and layers in the model (`hidden_layer_sizes`). For example, add one more layer and change the number of neurons in layers to 100, 20, 10. How has the decision boundary changed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPksSnFDCUxs"
      },
      "source": [
        "Calculate the predictions, accuracy and display the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-lkY0ypP60F"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m5GpHQHSldE"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MZ8IKE74ugw"
      },
      "source": [
        "## Comparing the classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6T0MA5G4Ifx"
      },
      "source": [
        "Now, apply the `LogisticRegression` and `MLPClassifier` models to the same dataset that we used with `DecisionTreeClassifier`. Which model gives better results (accuracy)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCPUUyUV4ZOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb0e2d0-ecba-4688-b013-b5d918a0a155"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer_dataset = load_breast_cancer()\n",
        "breast_cancer_dataset_pd = pd.DataFrame(breast_cancer_dataset.data, columns=breast_cancer_dataset.feature_names)\n",
        "X = breast_cancer_dataset_pd\n",
        "y = breast_cancer_dataset.target\n",
        "\n",
        "breast_cancer_dataset_pd.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoBmVzYj4kKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e06848-cadf-46a3-cd66-d0333b4c8036"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
        "# split the X and y matrices into train and test sets\n",
        "lr = LogisticRegression(max_iter=5000) \n",
        "?? # train LR model on the train set\n",
        "nn = MLPClassifier()\n",
        "?? # train MLP model on the train set\n",
        "?? # print the accuracy on the test set for both models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `` not found.\n",
            "Object `` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRFpLPTm8LTR"
      },
      "source": [
        "Display the weights for the features in the LR model (sorted by their positive impact)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8wxX4l07pTi"
      },
      "source": [
        "pd.Series(lr.coef_[0], index=breast_cancer_dataset.feature_names).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RrA6HjZi8CO"
      },
      "source": [
        "## Hand-written digits classification\n",
        "\n",
        "Next, we will compare the linear and non-linear models on the task of hand-writtem digit image classification. We will use a popular MNIST dataset from:\n",
        "https://www.openml.org/d/554\n",
        "\n",
        "Let's plot examples of images for each each digit from this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xreYIZYZx0QG"
      },
      "source": [
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "for i in range(10):\n",
        "  plt.imshow(X[y==str(i)][0].reshape(28, 28))\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyKYswuM5Djy"
      },
      "source": [
        "First, we will reshape the digits (pixels from each image) into a flat numerical vector - these will be the features for the model.\n",
        "\n",
        "We will split the dataset into train and test splits (using a sample of 10000 examples for faster computations). We will use a standard scaler to scale the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOnwn6083Ltn"
      },
      "source": [
        "X = X.reshape((X.shape[0], -1))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=10000,\n",
        "                                                    test_size=2000)\n",
        "scaler = StandardScaler()\n",
        "lr = LogisticRegression(penalty='l1', solver='saga', tol=0.1, C=0.1)\n",
        "image_classification_pipeline = make_pipeline(scaler, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKrkFJo95TOm"
      },
      "source": [
        "?? # fit the pipeline on the training set\n",
        "?? # predict classes for the test set\n",
        "?? # print accuracy for the test set\n",
        "?? # display the confusion matrix "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOizx7135xGk"
      },
      "source": [
        "### Classifier weights analysis\n",
        "\n",
        "One advantage of a linear classifier is its interpretability - we can plot the weights of the features to learn which ones have the strongest positive (blue) and negative (red) impact on the prediction for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp2B8TdygNEA"
      },
      "source": [
        "coef = lr.coef_.copy()\n",
        "scale = np.abs(coef).max()\n",
        "for i in range(10):\n",
        "  plt.imshow(coef[i].reshape(28, 28), \n",
        "           interpolation='nearest', cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McksxssU6aMD"
      },
      "source": [
        "Now, we will train a non-linear neural network model (multi-layer perceptron) and compare the results with the linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mvE4jXqvNIQ"
      },
      "source": [
        "nn =  MLPClassifier(solver='lbfgs', alpha=0.1, random_state=1, max_iter=2000,\n",
        "            early_stopping=True, hidden_layer_sizes=[10, 5])\n",
        "\n",
        "?? # create the pipeline with scaler and MultiPerceptron classifier\n",
        "?? # fit the pipeline\n",
        "?? # predict classes for the test set\n",
        "?? # print accuracy\n",
        "?? # display the confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvCp5vyL6o5g"
      },
      "source": [
        "The neural network should give a higher accuracy, but it is more difficult to interpret its weights (though there are some methods that enable this, eg. LIME which we used on the previous lab).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkubhmiAPAA"
      },
      "source": [
        "# What to remember\n",
        "* A linear model works well for *linearly separable*  classification problems.\n",
        "* A logistic regression model is a simple linear classifier which may be applied for such tasks. It learns weights for features that may be easily interpreted.\n",
        "* Deep neural network models consist of multiple units (neurons) that may be organized into layers. They can learn more complex (non-linear) features and achieve higher accuracy on complex tasks. However, it is more difficult to interpret their weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EhJRTFCBK5T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}