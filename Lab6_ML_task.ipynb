{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 6 - ML task.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "imRWJPsyrZWg"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imRWJPsyrZWg"
      },
      "source": [
        "## ML task\n",
        "\n",
        "In this lab, you will one of the ML tasks by applying the methods that we learned in the previous classes. You can use **one** of the prepared datasets from UCI ML Repository or choose another dataset (eg. from Kaggle). \n",
        "\n",
        "Below are some questions and tips to help you in designing the experiments and summarizing the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WN9zINWcKUN"
      },
      "source": [
        "! pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHXAraNaknwQ"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image \n",
        "import pydotplus\n",
        "from sklearn.externals.six import StringIO  \n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import train_test_split \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from matplotlib.colors import ListedColormap\n",
        "import math \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_blobs, make_regression\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "import random\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import urllib.request\n",
        "import zipfile\n",
        "%matplotlib inline\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWBR1mTyd47E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-am1nv06d3QZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNa65IezEJvj"
      },
      "source": [
        "def plot_decision_tree(model, feature_names):\n",
        "  dot_data = StringIO()\n",
        "  export_graphviz(model, out_file=dot_data,  \n",
        "                  filled=True, rounded=True, \n",
        "                  feature_names=feature_names)\n",
        "  graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "  return Image(graph.create_png())\n",
        "\n",
        "def display_confusion_matrix(y_test, y_pred):\n",
        "  confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
        "  confusion_matrix.index.name = 'Actual'\n",
        "  confusion_matrix.columns.name = 'Predicted'\n",
        "  sns.heatmap(confusion_matrix, annot=True)\n",
        "\n",
        "def plot_wordcloud(texts: list, title: str=''):\n",
        "  wc = wordcloud.WordCloud(background_color=\"white\").generate(' '.join(texts))\n",
        "  plt.figure()\n",
        "  plt.imshow(wc)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def preprocess_tokens(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.tokenize.regexp_tokenize(text, '[a-zA-Z]{3,}')\n",
        "    return [lemmatizer.lemmatize(word).lower() for word in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo1gqRvhCF94"
      },
      "source": [
        "## Problem definition\n",
        "\n",
        "* What is the goal of the prediction?\n",
        "* What problem does it solve?\n",
        "* What type of ML task is it (classification/regression)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6WrJCzckqqV"
      },
      "source": [
        "### SMS Spam Collection Data Set - text classification\n",
        "\n",
        "A text classification task: predict if a given SMS is spam or not, based on its text. (You can try to classify the texts without any cleaning as the stopwords and typos may have some impact on prediction)\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl5Mkw5twHGa"
      },
      "source": [
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "urllib.request.urlretrieve(data_url, 'smsspamcollection.zip')\n",
        "data_file = zipfile.ZipFile('smsspamcollection.zip')\n",
        "dataset = pd.read_csv(data_file.open('SMSSpamCollection'), delimiter=\"\\t\", header=None)\n",
        "dataset.columns = [\"spam\", \"text\"]\n",
        "X = dataset[\"text\"]\n",
        "y = dataset[\"spam\"]\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1flA9H4DXn70"
      },
      "source": [
        "### Seoul Bike Sharing Demand Data Set - regression\n",
        "Predict the number of bikes rented on given hour\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjIVwxbB4mcy"
      },
      "source": [
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv\"\n",
        "urllib.request.urlretrieve(data_url, 'SeoulBikeData.csv')\n",
        "dataset = pd.read_csv('SeoulBikeData.csv', sep=',', encoding = \"ISO-8859-1\")\n",
        "dataset[\"Holiday\"] = (dataset[\"Holiday\"] == \"Holiday\").astype(int)\n",
        "dataset[\"Functioning Day\"] = (dataset[\"Functioning Day\"] == \"Yes\").astype(int)\n",
        "X = dataset[['Hour', 'Temperature(°C)', 'Humidity(%)',\n",
        "       'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)',\n",
        "       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', \n",
        "       'Holiday', 'Functioning Day']]\n",
        "y = dataset[\"Rented Bike Count\"]\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K5wH2-foGGV"
      },
      "source": [
        "### Wine Quality Data Set - classification/regression\n",
        "\n",
        "The predicted value is wine quality grade - you can predict it as a continuous value (regression) or a discrete one (classification).\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QckHwcTnD5_"
      },
      "source": [
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
        "urllib.request.urlretrieve(data_url, 'winequality-white.csv')\n",
        "dataset = pd.read_csv('winequality-white.csv', sep=';')\n",
        "X = dataset[dataset.columns[:-1]]\n",
        "y = dataset[\"quality\"]\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvTiMyTzCewF"
      },
      "source": [
        "## Exploratory data analysis\n",
        "*  How many records are in the dataset?\n",
        "  `df.shape`\n",
        "*   What is the distribution of the target value?\n",
        "`df.describe()`, `sns.histplot(y)`, `sns.countplot(y)`\n",
        "* Are there any missing values? `df.isnull().sum()`\n",
        "* (For numerical data) - are there any correlations between the variables? `sns.heatmap(df.corr())`\n",
        "* (For text data) - What are the most common words for each class? `plot_wordcloud`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ffiVEvCgpv"
      },
      "source": [
        "?? # display the number of records (dataset shape)\n",
        "?? # plot the distributuon or calculate the dataset statistics\n",
        "?? # display the missing values summary\n",
        "?? # display the correlations or plot the word clouds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTGgyB95CmEG"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "  * Did you use any preprocessing? (eg. `StandardScaler` for numerical data, `CountVectorizer` for text, `make_pipeline` for combining the stages with the model)\n",
        "  * Which models did you use? (Select at least 2, including one interpretable model: `DecisionTreeRegressor`, `RandomForestRegressor`, `LinearRegression`, `MLPRegressor` for regression or \n",
        "`DecisionTreeClassifier`, `RandomForestClassifier`, `MLPClassifier`, `LogisticRegression` for classification)\n",
        "  * What hyper-parameters did you search? `GridSearchCV`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qeuu9O5CsQ0"
      },
      "source": [
        "?? # define the model or pipeline stages\n",
        "?? # define the parameter grid\n",
        "?? # create GridSearchCV object\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEPXniKjcpFp"
      },
      "source": [
        "?? # split data into train and test sets\n",
        "?? # fit the search object on the train set\n",
        "?? # generate predictions on the test set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frFEdgYuCvgb"
      },
      "source": [
        "## Evaluation \n",
        "  * What evaluation metrics did you apply on the test set? (use `train_test_split` and `metrics.accuracy_score`, `display_confusion_matrix` for classification or `metrics.mean_squared_error` for regression)\n",
        "  * Which model and hyperparameters yielded best results? Which one would you choose?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw1bc6-sCwzW"
      },
      "source": [
        "?? # calculate the metrics for predicted and test y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IM1oGHcztd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvjvKfAqCxAl"
      },
      "source": [
        "## (Optional) Results explanation\n",
        "  * What features were the most important (globally in the decision tree or linear model or locally in LIME for selected examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61jwfMpVCxi4"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}