{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4 - NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N8aRBxZCbdj6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIXVRcjQvWHf"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "In this lab, we will preprocess and build models for textual data. We will learn how to clean, transform and classify texts and how to explain the predictions for particular cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8aRBxZCbdj6"
      },
      "source": [
        "#### Imports and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIwOPcJ4mA4D"
      },
      "source": [
        "! pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HncQpugSvmx6"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import random\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import urllib.request\n",
        "import zipfile\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p20t6gBP3vBF"
      },
      "source": [
        "def plot_wordcloud(texts: list, title: str=''):\n",
        "  wc = wordcloud.WordCloud(background_color=\"white\").generate(' '.join(texts))\n",
        "  plt.figure()\n",
        "  plt.imshow(wc)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def preprocess_tokens(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.tokenize.regexp_tokenize(text, '[a-zA-Z]{3,}')\n",
        "    return [lemmatizer.lemmatize(word).lower() for word in tokens]\n",
        "\n",
        "\n",
        "def get_top_terms(tfidf, document, top_n=10):\n",
        "    print(document[:100])\n",
        "    features = tfidf.get_feature_names()\n",
        "    terms_vec = tfidf.transform([document]).toarray()[0]\n",
        "    return [features[i] for i in np.argsort(terms_vec)[::-1][:top_n]\n",
        "            if terms_vec[i]>0]\n",
        "\n",
        "\n",
        "def display_confusion_matrix(y_test, y_pred, class_names=None):\n",
        "    confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
        "    confusion_matrix.index.name = 'Actual'\n",
        "    confusion_matrix.columns.name = 'Predicted'\n",
        "    if class_names:\n",
        "      confusion_matrix.columns = class_names\n",
        "      confusion_matrix.index = class_names\n",
        "    sns.heatmap(confusion_matrix, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJbRsH6bheo"
      },
      "source": [
        "## News category classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8dX76uyir_"
      },
      "source": [
        "First, we will perform a text classification task for the 20 news groups dataset: http://qwone.com/~jason/20Newsgroups/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG_6l25svRhK"
      },
      "source": [
        "news_group_data = fetch_20newsgroups()\n",
        "class_names = news_group_data.target_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFlq9Dfe1Dws"
      },
      "source": [
        "news_group_pd = pd.DataFrame(news_group_data.data, columns=['text'])\n",
        "news_group_pd['category'] = news_group_data.target\n",
        "news_group_pd['category_name'] = news_group_pd['category'].apply(lambda x: news_group_data.target_names[x])\n",
        "news_group_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KKvVhbF1DlW"
      },
      "source": [
        "### Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4keXsn60Cyx"
      },
      "source": [
        "To better visualize the dataset, let us first plot the distribution of texts in classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmjMmWF0Kc3"
      },
      "source": [
        "sns.countplot(data=news_group_pd, y=\"category_name\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTn7MVje1C6e"
      },
      "source": [
        "Next, we want to explore the content of texts in each of the categories. We will plot the word clouds which show the words that occur most often in each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaBjcob80PNr"
      },
      "source": [
        "for category in class_names:\n",
        "  category_news_data = news_group_pd[news_group_pd['category_name']==category]\n",
        "  plot_wordcloud(texts=category_news_data['text'].to_list(), title=category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ2aHtfs1etM"
      },
      "source": [
        "We can see that the texts contain much of the noise and unnecessary information (such as the email subject)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymaPPAv3Q2M1"
      },
      "source": [
        "### Tokenization and vectorization\n",
        "\n",
        "Next, we need to prepare our dataset for the modeling task. To use the texts a an input to a ML model, we need to encode it as numerical vectors. We tokenize the texts (split into words) and build a \"Bag of Words\" representation. Each text is encoded as a vector in which the positions tokens from the vocabulary and the values represent the number of occurencies in this text.\n",
        "\n",
        "We will use the `CountVectorizer` class from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-4zoanxV8Z"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(news_group_pd['text'])\n",
        "print(\"Number of words in the vocabulary: \", len(vectorizer.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On637QqNStox"
      },
      "source": [
        "We can check what words are in the vocabulary by the `.get_features_names` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctn8qwWMzwkq"
      },
      "source": [
        "vectorizer.get_feature_names()[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg7HeJf3MT2F"
      },
      "source": [
        "Let us display the distribution of word frequencies (in how many document each word occurs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs6lpgtfM5PR"
      },
      "source": [
        "pd.DataFrame(vectors.sum(0)).iloc[0].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMo3ZC1KTEn2"
      },
      "source": [
        "### Text cleaning\n",
        "\n",
        "We can see that there are many tokens that occur in very few documents (possibly the noise) and some that occur in most of the texts. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKoWXuuIVAfH"
      },
      "source": [
        "#### Stopwords removal\n",
        "There are some words that are very common in the language but do not carry much information. Such words are called \"stopwords\" and we can list them for English language (based on the `nltk` library)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_reTcGT3SKx"
      },
      "source": [
        "set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ivzPldT583"
      },
      "source": [
        "To reduce the noise in the data, we will remove the stopwords and restrict the tokens to only alphabetic characters of at least 3 letters. Moreover, we will remove the words that occur in less than 10 documents or more than 90% (these are possibly the stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mpeBpHV3WS3"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', token_pattern='[a-zA-Z]{3,}',\n",
        "                             max_df=0.9, min_df=5)\n",
        "vectorizer.fit(news_group_pd['text'])\n",
        "print(vectorizer.get_feature_names()[:20])\n",
        "print(\"Number of words in the vocabulary: \", len(vectorizer.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMfMLYU6UzR2"
      },
      "source": [
        "#### Base form\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY6FVjUAUefu"
      },
      "source": [
        "We can also observe that some words in the dictionary have different grammatical forms. We can reduce the number of tokens by changing them to the base form - we can do it by stemming or lemmatization.\n",
        "\n",
        "Stemming cuts the ending of a word according to the language rules (fast but less accurate).\n",
        "\n",
        "Lemmatization finds the base form in a dictionary (more accurate but slower and requires external resources). \n",
        "\n",
        "We can see the difference between these approches on an example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-RP-DmV3g86"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = 'studies'\n",
        "stemma = stemmer.stem(word)\n",
        "lemma = lemmatizer.lemmatize(word)\n",
        "print(\"Word: {}, stemma: {}, lemma: {}\".format(word, stemma, lemma))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEA98DJqVwHh"
      },
      "source": [
        "We will apply all the preprocessing operations with one help function `preprocess_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vrxPMXy4EpD"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', tokenizer=preprocess_tokens,\n",
        "                             max_df=0.9, min_df=10)\n",
        "vectorizer.fit(news_group_pd['text'])\n",
        "print(vectorizer.get_feature_names()[:20])\n",
        "print(\"Number of words in the vocabulary: \", len(vectorizer.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqY_Y7aQZdM_"
      },
      "source": [
        "### Text classification\n",
        "\n",
        "Next, we will perform classification of the prepared texts. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ippvo6Z7i-"
      },
      "source": [
        "#### Text classification pipeline\n",
        "\n",
        "We will use a random forest classifier. The vectorizer and random forest will be combined as stages in a pipeline - thi means that the output of one step will used as input to the next one: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNmOizk8Nmo"
      },
      "source": [
        "text_classification_pipeline = make_pipeline(vectorizer, RandomForestClassifier(max_depth=20))\n",
        "print(text_classification_pipeline.named_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJYJh5cNZ5nq"
      },
      "source": [
        "We will split the data into train and test sets and perform the grid search for the hyperparameters of both steps in the pipeline. This time we will use the `RandomizedSearchCV` instead of standard grid search to sample only a subset of possible combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ybY9Rrj8kG-"
      },
      "source": [
        "train, test = train_test_split(news_group_pd, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z3h8iofswKq"
      },
      "source": [
        "param_grid = {\n",
        "    'countvectorizer__max_df': [0.5, 0.8, 1.0],\n",
        "    'randomforestclassifier__min_samples_leaf': [1, 10], \n",
        "}\n",
        "search = GridSearchCV(text_classification_pipeline, param_grid, n_jobs=-1)\n",
        "search.fit(train['text'], train['category_name'])\n",
        "print(search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUOaqyoB-cR6"
      },
      "source": [
        "y_pred = search.predict(test['text'])\n",
        "print(\"Accuracy: \", metrics.accuracy_score(test['category_name'], y_pred))\n",
        "display_confusion_matrix(test['category_name'], y_pred, class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5qaqQ7waoq9"
      },
      "source": [
        "### Model explanation\n",
        "\n",
        "As the random forest classifier is difficult to intepret, we will use a separate approach to explain the model's decisions. The LIME method build a local approximation of a complex model to explain why an instance was classified to given category: https://github.com/marcotcr/lime\n",
        "\n",
        "You can read more about LIME and and ML explanation methods: https://christophm.github.io/interpretable-ml-book/lime.html\n",
        "\n",
        "\n",
        "We will use the text explainer to highlight the words with highest impact on the classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic32KECkpEaR"
      },
      "source": [
        "explainer = LimeTextExplainer(class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmR9f6h5cUcd"
      },
      "source": [
        "First, we will display explanations for correctly classified examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efGZxjmUbwJD"
      },
      "source": [
        "correct_classes = test[y_pred == test['category_name']]\n",
        "correct_examples = correct_classes.sample(3)\n",
        "\n",
        "for i, example in correct_examples.iterrows():\n",
        "    exp = explainer.explain_instance(example[\"text\"], \n",
        "                                    search.predict_proba,\n",
        "                                      top_labels=2)\n",
        "    exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYkLfqz5cbd6"
      },
      "source": [
        "Next, we will display explanations for examples that were incorrectly assign to a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvvc6VjXcayh"
      },
      "source": [
        "incorrect_classes = test[y_pred != test['category_name']]\n",
        "incorrect_examples = incorrect_classes.sample(3)\n",
        "\n",
        "for i, example in incorrect_examples.iterrows():\n",
        "    print(\"Correct class: \", example[\"category_name\"])\n",
        "    exp = explainer.explain_instance(example[\"text\"], \n",
        "                                    search.predict_proba,\n",
        "                                      top_labels=2)\n",
        "    exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvGcKNR1off3"
      },
      "source": [
        "## Text sentiment classification\n",
        "\n",
        "Next, we will apply the same techniques to build a classifier for text sentiment (positive or negative). We will use the labelled dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pJ3kmhG_MYI"
      },
      "source": [
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "urllib.request.urlretrieve(data_url, 'sentiment%20labelled%20sentences.zip')\n",
        "data_file = zipfile.ZipFile('sentiment%20labelled%20sentences.zip')\n",
        "movie_reviews = pd.read_csv(data_file.open('sentiment labelled sentences/imdb_labelled.txt'), delimiter = \"\\t\", header=None)\n",
        "movie_reviews.columns=[\"text\", \"sentiment\"]\n",
        "class_names = [\"negative\", \"positive\"]\n",
        "movie_reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ulLzVrMu-c_"
      },
      "source": [
        "#### Display the word clouds for each category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zdLLRUPvX0O"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ddcpeIdt0i"
      },
      "source": [
        "### Fit the vectorizer \n",
        "Compare the number of tokens in vocabulary without any preprocessing and with preprocessing (use `max_df=0.9`, `min_df=10` and `preprocess_tokens` function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpCa3CYBeHf3"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9kPYg2vEiQ"
      },
      "source": [
        "#### Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Tr5_Akv0t1"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWtK53Kav6sh"
      },
      "source": [
        "#### Create the classification pipeline constisting of vectorizer and Random Forest Classifier steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dveLl2otv5xY"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUPKga17whj9"
      },
      "source": [
        "#### Configure the Grid Search \n",
        "Use the pipeline and parameters the same as for news group classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yyss1uqwGxR"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99BCltcNeqye"
      },
      "source": [
        "Display the accuracy and confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXchCgulwstC"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaA_LRhgeN68"
      },
      "source": [
        "### Explain the classification for 3 correctly and 3 incorrectly classified examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-1Nt8A3wyJq"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcTsxa7_w8nX"
      },
      "source": [
        "??"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}